\chapter{Introducción}
\label{chap:Introduccion}

\section{Contexto del proyecto}


\drop{E}{n} \cite{bernerslee2001semantic} se define la Web Semántica como
un tipo de Web cuyo contenido resulta procesable de manera automática. Para
conseguir un contenido procesable se debe antes llevar a cabo una
representación o formalización del conocimiento que se pretende exponer. Esto ha
sido posible gracias al concepto de \textit{tripla}
que no es más que la relación, expresada en un lenguaje formal, existente entre un sujeto y un objeto a través de
un predicado que los relaciona. De esta manera, si se desea expresar formalmente una sentencia
respecto de cualquier concepto bastará con establecer un sujeto o recurso (el
propio concepto) una relación o propiedad que se desee modelar, y un objeto que
a su vez puede ser un nuevo recurso de manera que agrupando conjuntos de triplas
se logra definir conceptos completos de manera formal. 


Durante los últimos años se han desarrollado lenguajes como \acs{RDF} y
\acs{OWL}, y
especificaciones, con el fin de dar soporte a 
la interoperabilidad semántica \cite{Shadbolt:2006:SWR:1155313.1155373}. En
1997 el \acf{W3C} ~\cite{W3CSW} define la primera especificación de \acs{RDF} lo que sentará los cimientos de la Web Semántica: el
objetivo de \acs{RDF} será aportar una descripción semántica del conocimiento en la
Web \cite{Shadbolt:2006:SWR:1155313.1155373} como lenguaje de definición de
recursos, haciendo posible trabajar con la totalidad de la Web
como un conjunto de recursos y sus interrelaciones. 

Posteriormente surge el paradigma Linked Data \cite{bernerslee:2009}, que se puede definir
como el uso del modelo Web para publicar datos estructurados de manera que
puedan ser fácilmente consumidos y combinados con otros ~\cite{bizer_linked_2009}. Valiéndose del concepto
de \acf{URI}, se consiguen identifcar recursos en la red de forma unívoca y así
poder enlazarlos sin ambigüedad. Linked Data
aprovecha la potencia descriptiva de \acs{RDF} para facilitar el procesado
y razonamiento automático sobre grandes conjuntos de datos. En
\cite{bernerslee:2009} se enumera una serie de principios que deben cumplir los
datos para ser considerados Linked Data: 


\begin{enumerate}
\item{Usar \acs{URI} como identificadores de los recursos publicados en la Web.}
\item{Usar las URL de estas \acs{URI} para que la gente pueda localizar y consultar
  estos recursos.}
\item{Proporcionar información útil cuando la \acs{URI} sea desreferenciada.}
\item{Incluir vínculos a otras \acs{URI} relacionadas con los datos en el recurso.}
\end{enumerate}


Por otra parte, el concepto de \textit{Calidad} puede aplicarse a cualquier tarea u objeto. Es
importante matizar para el contexto del proyecto qué se entiende por calidad y
más concretamente por calidad de los datos (\acs{DQ}). 


Existen varias definiciones de calidad que conviene considerar a la hora de
abordar el proyecto. En primer lugar, se va a exponer la definición que la Real
Academia Española de la lengua otorga a este término: 

\textit{Propiedad o conjunto de propiedades
  inherentes a algo que permiten juzgar su valor}.

Sin embargo este concepto ha ido variando con el tiempo, adaptándose a las
necesidades y a los procesos. Relacionando este concepto con la
\textit{fabricación} de un determinado bien, en ~\cite{ISHI} se define la calidad
como: 

\textit{Un producto tiene calidad cuando es desarrollado, diseñado y mantenido
  de la forma más económica, útil y satisfactoria para el consumidor}. 


Autores como \cite{conf/webist/CaballeroMACC} o \cite{conf/ekaw/FurberH10} dejan constancia de cómo la calidad de los datos resulta imprescindible en el
desarrollo con éxito de cualquier tarea o toma de decisión. Un nivel inadecuado de calidad en los datos puede tener impactos sustanciales en ámbitos
sociales y económicos. Las empresas están mejorando la calidad de los datos con
enfoques y herramientas prácticas \cite{Wang:1996:BAD:1189570.1189572}. 

En el ámbito de las tecnologías semánticas, la comunidad ha prestado poca
atención a la calidad de los datos que han sido representados mediante la
utilización de tecnologías semánticas
\cite{conf/ekaw/FurberH10}. Existen en la actualidad un número considerable de
frameworks disponibles para el desarrollo de aplicaciones basadas en el uso de
tecnologías semánticas, no obstante, ninguno de ellos incluye funcionalidades
específicas que permitan procesar una evaluación del nivel de calidad de los
datos que se están usando.

Puesto que ningún framework de desarrollo de Web Semántica facilita este tipo de
operaciones de medición, existe una necesidad de elaborar
mecanismos que permitan llevar a cabo evaluaciones de calidad de los datos
usados en aplicaciones de tecnología semántica en general y Web Semántica en
particular.

Con la irrupción de las tecnologías Big Data y dadas las necesidades que existen
de procesamiento distribuido sobre grandes volúmenes de información, encontramos
que el problema de la evaluación de la calidad de los datos semánticos se
extiende a este ámbito, no pudiendo encontrar ningún framework que permita
trabajar la calidad de los datos semánticos de manera distribuida. 

Considerando la no disponibilidad de operaciones de evaluación de calidad de datos en los
frameworks de desarrollo de tecnologías semánticas, el objetivo del \acf{TFM}
consiste en el desarrollo de una
extensión para uno de estos frameworks. La finalidad de
dicha extensión será la de diseñar, implementar y poner a disposición de la
comunidad un conjunto de primitivas de evaluación de calidad de datos para uno de
los frameworks de desarrollo de aplicaciones de tecnología semántica más
importantes, considerando el contexto en el que los
datos deben son usados. 


\section{Evolución respecto del PFC}


Tras llevar a cabo un estudio acerca del estado actual de los frameworks de
desarrollo de Web Semántica y Linked Data, durante el desarrollo del \acf{PFC}
se escogió uno de ellos basado en
Java, \textbf{Apache Jena}, y se extendió con la finalidad de ofrecer estos
mecanismos para la evaluación de la calidad de datos semánticos.

No obstante, durante los últimos años han tenido lugar una serie de cambios muy
significativos
en la industria de las tecnologías de la información, tales como el nacimiento
del Internet
de las Cosas, Computación en la Nube y la aparición de las Redes Sociales.

El desarrollo y explotación de estas tecnologías ha propiciado que la cantidad
de los datos
que se generan por unidad de tiempo se haya incrementado a una velocidad sin
precedentes,
trayendo consigo la necesidad de cómputo de tales volúmenes de información
heterogénea,
en tiempos razonables y de manera escalable \cite{map_reduce}.

Esta necesidad ha propiciado la aparición de paradigmas, tecnologías y numerosos
frameworks con los que abordar esta problemática desde diferentes puntos de
vista (procesamiento por lotes, en tiempo real, extracción, transformación y
carga de datos, seguridad,\ldots), agrupándose en ecosistemas tal y como pueda
ser el caso del \textbf{ecosistema Hadoop} \cite{HADOOP-ecosystem}.

\textbf{Apache Spark} \cite{SPARK} es un framework de procesamiento distribuido,
perteneciente al ecosistema Hadoop, que en los últimos años se ha convertido en
el estándar \textit{de facto} a la hora de abordar problemas de procesamiento de
grandes volúmenes de información en tiempos razonables, obteniendo unos
resultados de rendimiento varios órdenes de magnitud superior frente a modelos y
tecnologías anteriores, debido al procesamiento en memoria.

Mientras que el proyecto de Jena ha ido evolucionando para permitir el
procesamiento de datos semánticos de manera cetralizada y habiendo estudiado el
estado actual de los frameworks de procesamiento distribuido, se concluye que
ninguno de ellos ofrece primitivas para el procesamiento de datos semánticos en
distribuido y por lo tanto, tampoco mecanismos para llevar a cabo cálculos de
métricas de calidad sobre datos semánticos en entornos distribuidos.

Debido a la necesidad de incluir primitivas de evaluación de calidad de los
datos a la hora de desarrollar cualquier tarea o toma de decisión y puesto que
el ritmo de crecimiento de los datos dificulta su análisis en tiempos
razonables, se establece como objetivo principal de este Trabajo Fin de Máster
ofrecer un stack tecnológico basado en un framework de procesamiento distribuido
para que permita llevar a cabo análisis de calidad de datos semánticos en
entornos Big Data.

Para el presente Trabajo fin de Máster, será \textbf{Apache Spark} el framework
sobre el cual se construirá dicho stack para dar soporte a primitivas de
evaluación de calidad de los datos de una manera distribuida, escalable y
tolerante a fallos.

Como incremento respecto al \acf{PFC}, cabe destacar los siguientes aspectos:

\begin{enumerate}
\item Elaboración de un stack tecnológico basado en un framework de desarrollo
  distribuido, \textbf{Apache Spark}, para permitir el procesado de datos RDF:
  \textbf{SparkRDF}. Este stack estará basado en el framework \textbf{Apache
    Jena} para el procesamiento de datos en formatos semánticos y en
  \textbf{Apache Spark} para ofrecer primitivas de procesamiento en
  distribuido.
\item Extensión de dicho stack para permitir evaluación de calidad de datos
  semánticos de manera distribuida, \textbf{SparkDQ}.
\item Elección o desarrollo de una ontología que describa evaluaciones de
  calidad de los datos.
\item Desarrollo de primitivas de razonamiento sobre evaluaciones de
  calidad.
\item Incremento del rendimiento a la hora de calcular métricas sobre
  grafos de datos enlazados.
\item Estudio y comparativa entre resultados de \textbf{JenaDQ} y
  \textbf{SparkDQ} con el fin de establecer unos criterios de mejora
  en términos de tiempo y eficiencia a la hora de llevar a cabo
  estos cálculos de métricas de calidad.
\item Elaboración de una prueba de concepto de extracción de
  grandes volúmenes de datos, evaluación y presentación de las
  métricas.
\end{enumerate}

\section{Logro de competencias planteadas}

\texttt{TODO}

Pues ha ido flaman.

\section{Estructura del documento}

La forma en la que se ha organizado el presente documento se expone a
continuación: 

\begin{definitionlist}
\item[Capítulo \ref{chap:Introduccion}: \nameref{chap:Introduccion}] En este
  apartado se introduce el problema y se plantea el trabajo en base a los
  objetivos fijados. 


\item[Capítulo \ref{chap:objetivos}: \nameref{chap:objetivos}] En este capítulo
  se describe el principal objetivo del \acs{TFM} así como un conjunto de
  objetivos parciales que es necesario alcanzar para cubrir el objetivo
  final. 

\item[Capítulo \ref{chap:estadoarte}: \nameref{chap:estadoarte}] Esta sección
  introducirá los conceptos más importantes tratados en el documento y el
  proyecto, acompañado de conceptos y definiciones. Incluirá referencias al
  ámbito de la calidad de los datos, la Web Semántica y las tecnologías
  asociadas a ésta. 

\item[Capítulo \ref{chap:metodologia}: \nameref{chap:metodologia}] Se expondrá
  detalladamente cómo se ha utilizado la metodología de desarrollo elegida para la realización del \acs{TFM}.

\item[Capítulo \ref{chap:resultados}: \nameref{chap:resultados}] En este
  capítulo se expondrán los resultados obtenidos durante las diferentes
  iteraciones que se han llevado a cabo durante el desarrollo del
  \acs{TFM}. 

\item[Capítulo \ref{chap:conclusiones}: \nameref{chap:conclusiones}] En este
  capítulo se expondrán las conclusiones que se han obtenido tras la realización
  de este \acs{TFM} además de una serie de propuestas de trabajo futuro. 

\item[Anexo \ref{chap:ontologia}: \nameref{chap:ontologia}] Se
  corresponde con el listado de código de las ontologías desarrolladas para formalizar
  los resultados de evaluaciones de calidad de datos enlazados. 

\end{definitionlist}




