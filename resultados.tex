\chapter{Resultados}
\label{chap:resultados}

\drop{E}{n} este capítulo se presentan los resultados obtenidos tras llevar a
cabo el plan de trabajo presentado en el capítulo anterior. El proceso ha
desembocado en la obtención de varios artefactos, principalmente SparkDQ y su
prueba de concepto, tal y como se pretendía desarrollar.

\section{Fase de Inicio}

Durante la fase inicial, se destina una iteración a planificar el desarrollo del
\acs{TFM}, establecer requisitos y en función de éstos extraer una serie de
casos de uso.

Las tablas expuestas en el capítulo anterior se van a exponer de nuevo con el
fin de facilitar la lectura.

\subsection{Iteración 1: Planificación, casos de uso y requisitos}

La primera iteración tiene como objetivo establecer una planficación, detallar
requisitos y establecer una serie de casos de uso.

Se han obtenido como resultados los siguientes artefactos:

\begin{itemize}
\item Una revisión del estado del arte que puede consultarse en el Capítulo
  \ref{chap:estadoarte}, reflejando la situación actual en los principales
  campos relacionados con el \acs{TFM}
  \begin{itemize}
  \item Web Semántica
  \item Procesamiento en distribuido
  \item Calidad de Datos
  \item Calidad de Linked Data
  \end{itemize}

\item Una especificación de requisitos, funcionales y no funcionales, tanto para
  SparkDQ como para la prueba de concepto a realizar a partir de ésta.
\item Modelo general de Casos de Uso de SparkDQ y PoC
\item Planificación detallada dividida en fases e iteraciones del \acs{PUD}
\end{itemize}

\input{tables/iter1.tex}

\subsubsection{Requisitos}

Los requisitos funcionales de cualquier sistema software son aquellos que definen el comportamiento del
sistema y establecen un punto de partida para elaborar un modelo de casos de
uso, mientras que los no funcionales son aquellos orientados a aspectos del
sistema que notienen relación con el comportamiento, por ejemplo, el diseño, la
implementación on la usabilidad. 

Seguidamente se exponen los requisitos funcionales que se han identificado en la
primera fase del \acs{PUD}. Estos requisitos corresponden al desarrollo del
stack \textbf{SparkDQ}.

\begin{enumerate}
\item \textbf{RF 1. SparkDQ debe permitir la carga de triplas semánticas en un entorno
  distribuido} \\La premisa inicial es que el volumen de datos es
  suficientemente grande como para que se requiera un almacenamiento y
  procesamiento en distribuido de los mismos. Por lo tanto, SparkDQ debe ser
  capaz de leer las triplas almacenadas en entornos distribuidos y cargarlas en
  un modelo igualmente distribuido para su posterior procesamiento.
  
\item \textbf{RF 2. SparkDQ debe permitir al usuario modelar la evaluación de
  calidad}\\Se entiende como evaluación de calidad el cálculo de unas métricas y
  su posterior interpretación, así pues SparkDQ debe exponer mecanismos de
  evaluación de calidad de datos para ser llevados a cabo sobre un juego de
  datos distribuido. 
\item \textbf{RF 3. SparkDQ debe permitir llevar a cabo evaluaciones de calidad de
  datos enlazados para la métrica \textit{SchemaCompleteness}}\\Se evaluará esta
  métrica considerando un esquema deseable para los datos obteniendo como
  resultado un grado de alineamiento de los datos con dicho esquema. 

\item \textbf{RF 4. SparkDQ debe permitir llevar a cabo evaluaciones de calidad de
  datos enlazados para la métrica \textit{InterlinkingCompleteness}}\\Se
  evaluará esta métrica considerando un nivel de profundidad, siendo esto una
  agregación del nivel de interconectividad estratificado. 
\end{enumerate}

Una vez definidos los funcionales, se indican los no funcionales para SparkDQ.

\begin{itemize}
\item \textbf{RNF 1. Utilización de un framework para desarrollo de tecnologías
  semánticas: Apache Jena}\\Se retomará el uso de Apache Jena como framework de
  desarrollo de tecnologías semánticas para utilizar en el contexto del \acs{TFM}
\item \textbf{RNF 2. Utilización de un framework de procesamiento en
  distribuido: Apache Spark}\\Para el procesamiento distribuido se utilizará
  Apache Spark, que queda explicado en la sección \ref{sec:eco-hadoop}
\item \textbf{RNF 3. Generación de la documentación de la \acs{API}}\\Finalmente
  se generará la documentación de la API que permita a los desarrolladores
  extender y utilizar el módulo creado. 
\end{itemize}

Finalmente se exponen los requisitos funcionales y no funcionales identificados para la
aplicación de Prueba de Concepto.

\begin{enumerate}
\item \textbf{PoC debe permitir la adquisición de datos semánticos}\\Como parte
  del \textit{end to end} el primer paso de la prueba de concepto será
  seleccionar una fuente de datos y llevar a cabo un proceso de ingesta dentro
  de la arquitectura propuesta. 
  
\item \textbf{PoC debe permitir el almacenamiento de datos semánticos en un
  entorno distribuido}\\Los datos deben ingestarse en un entorno distribuido
  para su posterior consumo, considerando que su volumen y variedad pueden
  cambiar a lo largo del tiempo. 
\item \textbf{PoC debe permitir cargar datos semánticos desde un entorno
  distribuido y procesarlos}\\Los datos almacenados deben ser fácilmente
  accesibles y consumidos, entendiendo el consumo de estos datos como la
  adquisición, procesamiento y obtención de resultados en base a éstos. 
\item \textbf{PoC debe mantener unos niveles de seguridad adecuados}\\Los datos
  ingestados y almacenados deben estar protegidos de accesos no deseados. 
\item \textbf{PoC debe poder almacenar el resultado de las evaluaciones de
  calidad de datos en un entorno distribuido}\\Los resultados de las
  evaluaciones deben ser convenientemente almacenados bajo las mismas
  directrices de disponibilidad y seguridad que los datos inicialmente ingestados.
\item \textbf{PoC debe permitir el consumo de los resultados de las evaluaciones
  llevadas a cabo}\\Los resultados de las evaluaciones deben ser accesibles y
  fácilmente consumibles por parte de los usuarios. 
\end{enumerate}

Siendo los requisitos no funcionales los siguientes:

\begin{itemize}
\item \textbf{RNF 1. Arquitectura en Cloud}\\Se debe realizar una prueba de
  concepto utilizando tecnologías Cloud recientes y de uso extendido. 

\item \textbf{RNF 2. Utilización de parte del ecosistema Apache}\\Siendo SparkDQ
  un incremento sobre un framework del ecosistema Hadoop, se buscará integrarse
  también con otras tecnologías del mismo ecosistema. 
\end{itemize}


\subsubsection{Modelo general de Casos de Uso}

[INSERTE DIAGRAMA AQUÍ]

\section{Fase de Elaboración}

En la fase de Elaboración, se han sentado las bases para el desarrollo
propiamente dicho de los artefactos software que se han indicado en la fase
inicial. Concretamente se ha trabajado en dos vertientes:

\begin{itemize}
\item Actualización de JenaDQ como artefacto software
\item Diseño del stack SparkDQ
\end{itemize}

\subsection{Iteración 2: Actualización de JenaDQ}

\input{tables/iter2.tex}

En esta iteración el objetivo principal es actualizar JenaDQ y alinearlo con las
tecnologías que van a ser usadas en este proyecto.

Concretamente, se ha generado
un proyecto Maven (véase Sección\ref{maven}) con el que gestionar más fácilmente
las dependencias y el ciclo de vida de las aplicación.

Posteriormente se ha hecho una revisión del código y se han actualizado aquellos
puntos de JenaDQ que se han creído oportunos:

\begin{enumerate}
\item Se han añadido más pruebas unitarias.
\item Refactorizado de código.
\item Test de integración.
\end{enumerate}

Como resultado se ha obtenido una nueva versión de JenaDQ, que ha mantenido las
métricas pero cuyo nivel de calidad como artefacto software y mantenibilidad se
ha visto incrementado.

\subsection{Iteración 3: Diseño de la arquitectura de SparkDQ}

\input{tables/iter3.tex}

La iteración ha tenido como objetivo:

\begin{itemize}
\item Establecer un diseño de la arquitectura de SparkDQ
\item Establecer un diseño de la arquitectura de la prueba de concepto
\end{itemize}

En primer lugar se ha concluido que SparkDQ contará con dos elementos:
\begin{itemize}
\item \textbf{SparkRDF}: un artefacto encargado únicamente de la captura de datos
  semánticos y su transformación a una estructura de datos distribuida. En este
  caso, dicha estructura será la de un grafo dirigido. Este grafo se construirá
  sobre la librería Spark GraphX, orientada específicamente a trabajar con este
  tipo de estructuras.
  \item \textbf{SparkDQ}: artefacto encargado de llevar a cabo evaluaciones de
    calidad de datos sobre datos semánticos. Tendrá una dependencia con
    SparkRDF, delegando en éste la adquisición de datos y tomando en sus
    funciones como puntos de entrada los datos extraídos por SparkRDF. Se
    plantean a su vez dos métricas para evaluar: \textit{Interlinking} y
    \textit{SchemaCompleteness}, descritas en la sección \ref{schemacompleteness}. 
\end{itemize}

Para la prueba de concepto, se ha establecido:

\begin{enumerate}
\item Generación de un artefacto que lleve a cabo cálculos de calidad de datos
  semánticos sobre un conjunto suficientemente grande. Dicho artefacto se
  llamará spark-sem-dq-assessment y tendrá una dependencia de SparkDQ. Podrá
  configurarse para llevar a cabo una evaluación de la calidad de los datos
  sobre cualquiera de las métricas que ofrezca SparkDQ.
\item Elaboración de un diseño de arquitectura en cloud: involucrará los
  siguientes elementos:
  \begin{enumerate}
  \item Arquitectura de microservicios: para llevar a cabo la captura y
    procesamiento de datos. 
  \item Almacenamiento en cloud - Amazon S3: como repositorio de datos iniciales
    y directorio final de resultados de cálculos.
  \item Visualización - Amazon ElasticSearc and Kibana stack: para la
    visualización de los resultados de las evaluaciones.  
  \end{enumerate}
\end{enumerate}

Se ha obtenido como resultado el diagrama de clases de SparkRDF, diagrama de clases
de SparkDQ, diagrama de clases del job de la prueba de concepto 
spark-sem-dq-assessment y diagrama \textit{end to end} de la prueba de
concepto.

[INSERTAR DIAGRAMAS AQUÍ]


\section{Fase de Construcción}

Una vez asentados los diseños de los artefactos a desarrollar, en la fase de
construcción tendrá lugar el desarrollo del núcleo de la aplicación, esto es,
los artefactos clave de desarrollo: SparkRDF y SparkDQ, en las iteraciones 4 y 5
respectivamente. 

\subsection{Iteración 4: Desarrollo de SparkRDF}

\input{tables/iter4.tex}

SparkRDF utiliza un sub-módulo de Apache Jena (riot) para la lectura de
documentos en formato ttl, es decir, en forma de triplas, en las que cada línea
del fichero tiene tres campos separados por espacio, siendo dichos campos sujeto,
predicado y objeto, respectivamente.

\lstset{escapechar=@,language=scala}
\begin{lstlisting}[caption={Fragmento del lector de triplas},captionpos=b]
def loadTriplets(sparkSession: SparkSession, path: String): RDD[Triple] = {
  val tripletsRDD = sparkSession.sparkContext.textFile(path)
  .filter(line => !line.trim().isEmpty & !line.startsWith(``#''))
  .map(line =>
          RDFDataMgr.createIteratorTriples(new
          ByteArrayInputStream(line.getBytes), Lang.NTRIPLES, null).next())
          tripletsRDD
}  
\end{lstlisting}



\subsection{Iteración 5: Desarrollo de SparkDQ para la métrica \textit{Interlinking}}

\input{tables/iter5.tex}

[INSERTAR CHUNK Y TEORÍA SOBRE INTERLINKING]

\subsection{Iteración 6: Desarrollo de SparkDQ para la métrica \textit{SchemaCompleteness}}

\input{tables/iter6.tex}

[INSERTAR CHUNG Y TEORÍA SOBRE SC]

\section{Fase de Transición}

\subsection{Iteración 7: Desarrollo de la PoC}

\input{tables/iter7.tex}

\subsection{Iteración 8: Entrega de TFM}

\input{tables/iter8.tex}
